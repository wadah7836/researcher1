# scholar_app.py
from pywebio import start_server, input, output
import requests
from bs4 import BeautifulSoup
import os
import urllib.parse
import random
import time
from datetime import datetime

put_text = output.put_text
put_success = output.put_success
put_error = output.put_error
put_html = output.put_html

LOG_FILE = "scholar_log.txt"

USER_AGENTS = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.0 Safari/605.1.15",
    "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/118.0.5993.70 Safari/537.36",
]

def log_error(message):
    """ØªØ³Ø¬ÙŠÙ„ Ø£ÙŠ Ø®Ø·Ø£ ÙÙŠ Ù…Ù„Ù Ù„Ø³Ù‡ÙˆÙ„Ø© Ø§Ù„ØªØªØ¨Ø¹"""
    with open(LOG_FILE, "a", encoding="utf-8") as f:
        f.write(f"[{datetime.now()}] {message}\n")

def extract_author_id(scholar_url: str):
    if not scholar_url:
        return None
    parsed = urllib.parse.urlparse(scholar_url)
    q = urllib.parse.parse_qs(parsed.query)
    if "user" in q and q["user"]:
        return q["user"][0]
    if "user=" in scholar_url:
        try:
            part = scholar_url.split("user=")[1].split("&")[0]
            return part
        except Exception:
            return None
    return None

def fetch_via_requests(url: str, retries=3, delay=5):
    """ÙŠØ­Ø§ÙˆÙ„ Ø¬Ù„Ø¨ Ø§Ù„ØµÙØ­Ø© Ù…Ø¹ Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© ÙˆØªØ¨Ø¯ÙŠÙ„ User-Agent"""
    for attempt in range(1, retries + 1):
        try:
            headers = {
                "User-Agent": random.choice(USER_AGENTS),
                "Accept-Language": "en-US,en;q=0.9",
            }
            resp = requests.get(url, headers=headers, timeout=15)
            if resp.status_code == 200:
                return resp.text
            else:
                log_error(f"HTTP {resp.status_code} Ø£Ø«Ù†Ø§Ø¡ Ù…Ø­Ø§ÙˆÙ„Ø© {attempt}")
        except Exception as e:
            log_error(f"Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„Ø¬Ù„Ø¨ ÙÙŠ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© {attempt}: {e}")
        time.sleep(delay)
    raise Exception("ÙØ´Ù„ Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ø¹Ø¯ Ø¹Ø¯Ø© Ù…Ø­Ø§ÙˆÙ„Ø§Øª. Ù‚Ø¯ ÙŠÙƒÙˆÙ† Google Ø­Ø¸Ø±Ùƒ Ù…Ø¤Ù‚ØªÙ‹Ø§.")

def parse_publications(soup):
    """ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¨Ø­ÙˆØ« Ù…Ù† ØµÙØ­Ø© ÙˆØ§Ø­Ø¯Ø©"""
    publications = []
    for row in soup.select(".gsc_a_tr"):
        title_tag = row.select_one(".gsc_a_at")
        title = title_tag.text.strip() if title_tag else "Ø¨Ø¯ÙˆÙ† Ø¹Ù†ÙˆØ§Ù†"
        link = "https://scholar.google.com" + title_tag["href"] if title_tag else "#"
        authors_tag = row.select_one(".gsc_a_at+ .gs_gray")
        authors = authors_tag.text.strip() if authors_tag else ""
        journal_tag = row.select(".gs_gray")
        journal = journal_tag[1].text.strip() if len(journal_tag) > 1 else ""
        cited_tag = row.select_one(".gsc_a_c a")
        citations = cited_tag.text.strip() if cited_tag else "0"
        year_tag = row.select_one(".gsc_a_y span")
        year = year_tag.text.strip() if year_tag else "â€”"
        publications.append({
            "title": title,
            "authors": authors,
            "journal": journal,
            "citations": citations,
            "year": year,
            "link": link
        })
    return publications

def parse_soup_to_data(soup, base_url):
    """ØªØ­Ù„ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¨Ø§Ø­Ø« ÙˆØ¬Ù„Ø¨ ÙƒÙ„ Ø§Ù„Ø¨Ø­ÙˆØ«"""
    name_tag = soup.find("div", id="gsc_prf_in")
    name = name_tag.text.strip() if name_tag else "ØºÙŠØ± Ù…Ø¹Ø±ÙˆÙ"
    image_tag = soup.find("img", id="gsc_prf_pup-img")
    image_url = "https://scholar.google.com" + image_tag["src"] if image_tag else None

    affiliation = soup.find("div", class_="gsc_prf_il")
    affiliation = affiliation.text.strip() if affiliation else "ØºÙŠØ± Ù…Ø­Ø¯Ø¯"
    fields = [a.text.strip() for a in soup.select("#gsc_prf_int a")]
    email_tag = soup.find("div", class_="gsc_prf_ivh")
    email = email_tag.text.strip() if email_tag else "ØºÙŠØ± Ù…ØªÙˆÙØ±"

    citations_all = h_index_all = i10_index_all = "0"
    stats_table = soup.find("table", id="gsc_rsb_st")
    if stats_table:
        tds = stats_table.find_all("td", class_="gsc_rsb_std")
        if len(tds) >= 6:
            citations_all, _, h_index_all, _, i10_index_all, _ = [td.text for td in tds[:6]]

    # Ø¬Ù„Ø¨ ÙƒÙ„ Ø§Ù„Ø¨Ø­ÙˆØ« Ø¹Ø¨Ø± Ø§Ù„ØµÙØ­Ø§Øª
    publications = []
    start = 0
    while True:
        page_url = f"{base_url}&cstart={start}&pagesize=20"
        html = fetch_via_requests(page_url)
        page_soup = BeautifulSoup(html, "html.parser")
        new_pubs = parse_publications(page_soup)
        if not new_pubs:
            break
        publications.extend(new_pubs)
        start += 20
        time.sleep(1)  # ØªØ£Ø®ÙŠØ± Ø¨Ø³ÙŠØ· Ù„ØªØ¬Ù†Ø¨ Ø§Ù„Ø­Ø¸Ø± Ø§Ù„Ù…Ø¤Ù‚Øª

    return {
        "name": name,
        "affiliation": affiliation,
        "fields": fields,
        "email": email,
        "image": image_url,
        "citations": citations_all,
        "h_index": h_index_all,
        "i10_index": i10_index_all,
        "publications": publications,
    }

def fetch_full_scholar_data():
    # Ø´Ø¹Ø§Ø± Ø§Ù„Ù‡ÙŠØ¦Ø© ÙÙŠ Ø§Ù„Ø¨Ø¯Ø§ÙŠØ©
    put_html("""
        <h1 style='text-align:center; color:#004aad; margin-bottom:20px;'>Ù‡ÙŠØ¦Ø© Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø¹Ù„Ù…ÙŠ</h1>
        <hr>
    """)

    url = input.input("Ø£Ø¯Ø®Ù„ Ø±Ø§Ø¨Ø· Ø§Ù„Ø¨Ø§Ø­Ø« ÙÙŠ Google Scholar:", type="text")
    if not url:
        put_error("ÙŠØ±Ø¬Ù‰ Ø¥Ø¯Ø®Ø§Ù„ Ø±Ø§Ø¨Ø· Ø§Ù„Ø¨Ø§Ø­Ø«")
        return

    try:
        put_text("ğŸ” Ø¬Ø§Ø±ÙŠ Ø¬Ù„Ø¨ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† Google Scholar...")
        html = fetch_via_requests(url)
        soup = BeautifulSoup(html, "html.parser")
        data = parse_soup_to_data(soup, url)

        # Ø¹Ø±Ø¶ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¨Ø§Ø­Ø«
        html_card = f"""
        <div style='display:flex; align-items:center; gap:20px; margin-bottom:20px; margin-top:20px;'>
            <img src='{data.get('image')}' alt='ØµÙˆØ±Ø© Ø§Ù„Ø¨Ø§Ø­Ø«' width='120' style='border-radius:10px;'/>
            <div>
                <h2>{data.get('name')}</h2>
                <p><b>Ø§Ù„Ù…Ø¤Ø³Ø³Ø©:</b> {data.get('affiliation')}</p>
                <p><b>Ø§Ù„Ø¨Ø±ÙŠØ¯:</b> {data.get('email')}</p>
                <p><b>Ø§Ù„Ù…Ø¬Ø§Ù„Ø§Øª:</b> {', '.join(data.get('fields') or []) if data.get('fields') else 'ØºÙŠØ± Ù…Ø­Ø¯Ø¯'}</p>
            </div>
        </div>
        """
        put_html(html_card)

        stats_html = f"""
        <table border='1' cellpadding='8' style='border-collapse:collapse; margin-bottom:20px;'>
            <tr style='background:#f0f0f0'><th>Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ©</th><th>Ø§Ù„Ù‚ÙŠÙ…Ø©</th></tr>
            <tr><td>Ø§Ù„Ø§Ø³ØªØ´Ù‡Ø§Ø¯Ø§Øª</td><td>{data['citations']}</td></tr>
            <tr><td>h-index</td><td>{data['h_index']}</td></tr>
            <tr><td>i10-index</td><td>{data['i10_index']}</td></tr>
        </table>
        """
        put_html(stats_html)

        pubs = data.get("publications", [])
        total_pubs = len(pubs)
        put_html(f"<h3 style='color:#004aad;'>Ø¹Ø¯Ø¯ Ø§Ù„Ø¨Ø­ÙˆØ« Ø§Ù„ÙƒÙ„ÙŠ: {total_pubs}</h3>")

        pubs_html = """
        <table border='1' cellpadding='6' style='border-collapse:collapse; width:100%'>
            <tr style='background:#f0f0f0'>
                <th>Ø§Ù„Ø¹Ù†ÙˆØ§Ù†</th><th>Ø§Ù„Ù…Ø¤Ù„ÙÙˆÙ†</th><th>Ø§Ù„Ù…Ø¬Ù„Ø©</th><th>Ø§Ù„Ø§Ø³ØªØ´Ù‡Ø§Ø¯Ø§Øª</th><th>Ø§Ù„Ø³Ù†Ø©</th>
            </tr>
        """
        for p in pubs:
            pubs_html += f"""
            <tr>
                <td><a href='{p['link']}' target='_blank'>{p['title']}</a></td>
                <td>{p['authors']}</td>
                <td>{p['journal']}</td>
                <td>{p['citations']}</td>
                <td>{p['year']}</td>
            </tr>
            """
        pubs_html += "</table>"
        put_html(pubs_html)

        # Ø´Ø¹Ø§Ø± Ø§Ù„Ù‡ÙŠØ¦Ø© ÙÙŠ Ø§Ù„Ù†Ù‡Ø§ÙŠØ©
        put_html("""
            <hr>
            <h2 style='text-align:center; color:#004aad; margin-top:20px;'>Ù‡ÙŠØ¦Ø© Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø¹Ù„Ù…ÙŠ</h2>
        """)

    except Exception as e:
        log_error(f"Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„ØªÙ†ÙÙŠØ°: {e}")
        put_error(f"âŒ Ø­Ø¯Ø« Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„Ø¬Ù„Ø¨: {e}")

if __name__ == "__main__":
    port = int(os.environ.get("PORT", 5000))
    start_server(fetch_full_scholar_data, port=port, host="0.0.0.0")
